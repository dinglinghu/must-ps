"""
LiteLLMå®¢æˆ·ç«¯
ä¸ºADKæ¡†æ¶æä¾›ç»Ÿä¸€çš„å¤§æ¨¡å‹APIæ¥å£ï¼Œæ”¯æŒDeepSeekç­‰å¤šç§æ¨¡å‹
"""

import logging
import os
import asyncio
from typing import Dict, List, Any, Optional, AsyncGenerator
from datetime import datetime

logger = logging.getLogger(__name__)

try:
    import litellm
    from litellm import completion, acompletion
    LITELLM_AVAILABLE = True
    logger.info("âœ… LiteLLMå·²å¯¼å…¥")
except ImportError:
    LITELLM_AVAILABLE = False
    logger.warning("âš ï¸ LiteLLMæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install litellm")


class LiteLLMClient:
    """LiteLLMç»Ÿä¸€å®¢æˆ·ç«¯"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–LiteLLMå®¢æˆ·ç«¯
        
        Args:
            config: é…ç½®å­—å…¸
        """
        if not LITELLM_AVAILABLE:
            raise ImportError("LiteLLMæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install litellm")
        
        self.config = config
        self.model = config.get('model', 'deepseek/deepseek-chat')
        self.api_key = self._get_api_key()
        self.base_url = config.get('base_url')
        
        # è®¾ç½®LiteLLMé…ç½®
        self._setup_litellm()
        
        logger.info(f"âœ… LiteLLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ: {self.model}")
    
    def _get_api_key(self) -> Optional[str]:
        """è·å–APIå¯†é’¥"""
        # ä¼˜å…ˆä½¿ç”¨ç›´æ¥é…ç½®çš„APIå¯†é’¥
        api_key = self.config.get('api_key')
        if api_key:
            return api_key
        
        # ä»ç¯å¢ƒå˜é‡è·å–
        api_key_env = self.config.get('api_key_env')
        if api_key_env:
            api_key = os.getenv(api_key_env)
            if api_key:
                return api_key
            else:
                logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡ {api_key_env} æœªè®¾ç½®")
        
        return None
    
    def _setup_litellm(self):
        """è®¾ç½®LiteLLMé…ç½®"""
        try:
            # è®¾ç½®APIå¯†é’¥
            if self.api_key:
                if self.model.startswith('deepseek/'):
                    os.environ['DEEPSEEK_API_KEY'] = self.api_key
                    logger.info("âœ… è®¾ç½®DeepSeek APIå¯†é’¥")
                elif self.model.startswith('openai/') or 'gpt' in self.model:
                    os.environ['OPENAI_API_KEY'] = self.api_key
                    logger.info("âœ… è®¾ç½®OpenAI APIå¯†é’¥")
            
            # è®¾ç½®åŸºç¡€URLï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.base_url:
                litellm.api_base = self.base_url
                logger.info(f"âœ… è®¾ç½®APIåŸºç¡€URL: {self.base_url}")
            
            # è®¾ç½®æ—¥å¿—çº§åˆ«
            litellm.set_verbose = False  # å‡å°‘æ—¥å¿—è¾“å‡º
            
        except Exception as e:
            logger.error(f"âŒ LiteLLMé…ç½®è®¾ç½®å¤±è´¥: {e}")
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        agent_name: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨èŠå¤©å®ŒæˆAPI
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            APIå“åº”
        """
        try:
            # å‡†å¤‡å‚æ•°
            params = {
                'model': self.model,
                'messages': messages,
                'temperature': temperature or self.config.get('temperature', 0.7),
                'max_tokens': max_tokens or self.config.get('max_tokens', 4096),
                'stream': stream,
                **kwargs
            }
            
            # ç§»é™¤Noneå€¼
            params = {k: v for k, v in params.items() if v is not None}

            # æ„å»ºæ™ºèƒ½ä½“ä¿¡æ¯
            agent_info = f" (Agent: {agent_name})" if agent_name else ""

            logger.info(f"ğŸ”— LiteLLM APIè°ƒç”¨{agent_info}: {self.model}")
            logger.debug(f"ğŸ“ æ¶ˆæ¯æ•°é‡: {len(messages)}")

            # è°ƒç”¨LiteLLM
            if stream:
                return await self._stream_completion(params, agent_name)
            else:
                response = await acompletion(**params)
                logger.info(f"âœ… LiteLLM APIè°ƒç”¨æˆåŠŸ{agent_info}: {self.model}")
                return response
                
        except Exception as e:
            logger.error(f"âŒ LiteLLM APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def _stream_completion(self, params: Dict[str, Any], agent_name: Optional[str] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼å®Œæˆ"""
        try:
            response = await acompletion(**params)
            async for chunk in response:
                yield chunk
        except Exception as e:
            logger.error(f"âŒ LiteLLMæµå¼APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def generate_response(
        self,
        system_prompt: str,
        user_message: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        agent_name: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆå“åº”ï¼ˆç®€åŒ–æ¥å£ï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            
        Returns:
            ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        try:
            response = await self.chat_completion(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                agent_name=agent_name
            )
            
            if hasattr(response, 'choices') and len(response.choices) > 0:
                content = response.choices[0].message.content
                logger.info(f"âœ… å“åº”ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(content)}")
                return content
            else:
                logger.error(f"âŒ å“åº”æ ¼å¼å¼‚å¸¸: {response}")
                return "å“åº”æ ¼å¼å¼‚å¸¸"
                
        except Exception as e:
            logger.error(f"âŒ å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return f"è°ƒç”¨å¤±è´¥: {e}"
    
    async def generate_response_stream(
        self,
        system_prompt: str,
        user_message: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆå“åº”
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            
        Yields:
            æµå¼å“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        try:
            async for chunk in await self.chat_completion(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            ):
                if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        yield delta.content
                        
        except Exception as e:
            logger.error(f"âŒ æµå¼å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            yield f"æµå¼è°ƒç”¨å¤±è´¥: {e}"


def create_litellm_client(config: Dict[str, Any]) -> LiteLLMClient:
    """
    åˆ›å»ºLiteLLMå®¢æˆ·ç«¯
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        LiteLLMClientå®ä¾‹
    """
    if not LITELLM_AVAILABLE:
        raise ImportError("LiteLLMæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install litellm")
    
    logger.info(f"âœ… åˆ›å»ºLiteLLMå®¢æˆ·ç«¯: {config.get('model', 'unknown')}")
    return LiteLLMClient(config)


def test_litellm_connection(config: Dict[str, Any]) -> bool:
    """
    æµ‹è¯•LiteLLMè¿æ¥
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        è¿æ¥æ˜¯å¦æˆåŠŸ
    """
    try:
        client = create_litellm_client(config)
        
        # ç®€å•æµ‹è¯•
        async def test():
            response = await client.generate_response(
                system_prompt="ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹ã€‚",
                user_message="è¯·å›å¤'æµ‹è¯•æˆåŠŸ'",
                max_tokens=10
            )
            return "æµ‹è¯•æˆåŠŸ" in response or "success" in response.lower()
        
        result = asyncio.run(test())
        if result:
            logger.info("âœ… LiteLLMè¿æ¥æµ‹è¯•æˆåŠŸ")
        else:
            logger.warning("âš ï¸ LiteLLMè¿æ¥æµ‹è¯•å¤±è´¥")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ LiteLLMè¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
        return False
